## Vision and Language Pretraining
Pretrain on large V&L Dataset and finetune on downstream task like VQA2.0, NLVR2...

## Content
| Paper | Conference | short notes |
| :---: | :---: | :---: |
|[ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334)| ICML 2021 | [report](https://github.com/rentainhe/what_I_have_read/issues/2#issue-957270508) |
